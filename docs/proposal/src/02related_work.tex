\section{Related work}

\subsection{Curriculum Learning}
In the original formulation, \citet{Bengio2009CurriculumL} proposed curriculum learning as a method to gradually increase the complexity of the data samples used during the training process. Inspired by how humans learn, the model starts with simpler tasks and examples, gradually building towards more challenging ones. This approach aims to accelerate convergence, reduce resource consumption, and improve overall performance on complex tasks and examples. The central hypothesis is that by structuring the learning process, curriculum learning can lead to faster training and improved generalization, especially in tasks where traditional random sampling may slow down progress or lead to inefficient learning patterns.

\subsection{Chain of Thought (CoT)}
Chain of Thought, \citet{wei2023chainofthoughtpromptingelicitsreasoning}, is a series of intermediate reasoning steps and Chain of Thought prompting is a simple prompting technique that significantly improves the ability of large language models to perform complex reasoning and improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks, where a few chain of thought demonstrations are provided as exemplars in prompting. In essence, this works by prompting the LLM to explain its steps in solving a problem and encourages LLMs to show their reasoning by breaking down tasks into smaller pieces.

\subsection{Self Consistency (SC)}

Self Consistency \citet{wang2023selfconsistencyimproveschainthought} is an expansion on the Chain of Thought \cite{wei2023chainofthoughtpromptingelicitsreasoning} prompting method where the model's decoder is queried (via chain of thought) multiple times. This then generates a diverse set of possible reasoning paths that the model outputs. Finally, the correct answer is chosen by a majority/plurality vote of these outputs. However, this method works and is superior to chain-of-thought reasoning alone in finding the correct answer due to marginalization over the rationale in the chain of thought.

\subsection{Least to Most Prompting}

LLMs generally perform poorly when trying to solve problems that are harder than the examples shown in their training prompts. Least-to-Most \citet{zhou2023leasttomostpromptingenablescomplex} prompting is a technique to combat this where during the chain-of-thought reasoning stage the model will break a complex problem down into simpler subproblems, using the answers to the subproblems to then derive the answer to the more complex problem. Models prompted with this method are shown in \cite{zhou2023leasttomostpromptingenablescomplex} to be able to generalize solutions to more complex problems than they were trained on.

\subsection{Mixture of Experts (MoE)}
Mixture of Experts, \citet{jacobs1991adaptivemixtureoflocalexperts}, is a machine learning technique that divides a model into separate sub-models (or "experts"), each specializing in a subset of the input data, to jointly perform a task. Mixture of Experts architectures enable large-scale models, even those comprising many billions of parameters, to greatly reduce computation costs during pre-training and achieve faster performance during inference time. Broadly speaking, it achieves this efficiency through selectively activating only the specific experts needed for a given task, rather than activating the entire model for every task. It offers a means to address the tradeoff between the greater capacity of larger models and the greater efficiency of smaller models. This has been most notably explored in the field of natural language processing (NLP): some leading large language models (LLMs) like Mistral’s Mixtral 8x7B and (according to some reports) OpenAI’s GPT-4, have employed MoE architecture.