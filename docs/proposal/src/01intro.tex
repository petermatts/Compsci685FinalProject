\section{Introduction}

Large-language models have demonstrated remarkable proficiency in natural language processing tasks like text generation, machine translation, code generation, and question answering. Their abilities left much to be desired in the context of mathematical reasoning and arithmetic. While LLMs can generate syntactically correct responses to mathematical queries, they often struggle with logical consistency, multi-step reasoning, and generalization across different levels of mathematical difficulty. LLMs are approximate; they do not explicitly reason which often leads to inconsistencies in multi-step problem solving and difficulty in generalizing beyond unseen data. Addressing these limitations requires structured training methodologies that enhance the model’s ability to learn and apply mathematical concepts effectively. 
\\\\
One promising approach is Curriculum Learning (CL), a training paradigm that structures learning in a progressive manner, enabling models to acquire foundational knowledge before advancing to more complex concepts. In this study, we propose a curriculum-based fine-tuning strategy for improving an LLM’s mathematical reasoning abilities. Specifically, we aim to fine-tune the model in a sequential manner, first training it on grade school mathematics before introducing high school-level problems. By leveraging multiple datasets and structured learning techniques, we seek to improve the model’s capacity for logical reasoning and problem-solving. 
\\\\
In addition to Curriculum Learning, we explore the integration of advanced techniques, including Chain of Thought (CoT) prompting, which encourages step-by-step reasoning; Self-Consistency (SC), which enhances answer reliability through multiple reasoning paths; and a potential Mixture of Experts (MoE) framework to improve model efficiency and specialization. To evaluate the effectiveness of our approach, we design three experimental configurations: (1) direct fine-tuning on a high school math dataset; (2) sequential fine-tuning, where the model is first trained on grade school mathematics before progressing to high school content; and (3) an extended curriculum learning framework incorporating a pre-trained teacher model to guide learning at each stage. Recent advances in Chain of Thought prompting and Self Consistency have shown improvements in reasoning by encouraging the decomposition of a problem into smaller intermediary steps.  
\\\\
Through these experiments, we aim to determine the most effective strategy for enhancing LLMs’ mathematical reasoning capabilities and evaluate whether the efficacy of curriculum learning as a tool to enhance the mathematical abilities of LLMs. Our findings will contribute to the broader objective of improving AI-driven education, enabling LLMs to better assist in mathematical instruction, tutoring, and automated problem-solving applications. 


% Large language models approximate, no explicit reasoning mechanisms. 