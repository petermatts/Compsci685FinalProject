\section{Tools}

To gather resources and data for our experiments we will obtain pre-trained weights for our models from \href{https://huggingface.co/}{Hugging Face}. Both the GSM8K and MATH datasets are also available and will be obtained from Hugging Face.
\\\\
To run our code we will utilize the \href{https://pytorch.org/}{PyTorch} library likely using one (or a combination) of the following: Google Colabratory, AWS SageMaker (Amazon competitor to Google Colab), and our local (personal) compute resources utilizing the opensource API from \href{https://unsloth.ai/}{Unsloth}. Unsloth uses the Triton kernel developed by OpenAI to run LLMs on GPUs faster and more efficiently in terms of memory and time.


% What existing libraries or toolkits are you going to use? Some questions to think about: will you be doing any preprocessing of your data such as tokenization or parsing? Will you be training logistic regression models? Will you be using deep learning libraries (if not, you need to justify why)? Will you need to use any services for GPUs?\footnote{As we said in class, we strongly suggest \url{https://colab.research.google.com}!} Do you need to use crowdsourcing?