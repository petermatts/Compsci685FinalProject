\section{Related work}
Recent advances in natural language processing (NLP) and machine learning have increasingly focused on enabling models to perform complex reasoning tasks, particularly in domains like mathematical problem-solving and structured generation. This work builds on a rich history of research on model architectures, learning paradigms, and prompting techniques. The importance of learning paradigms was highlighted in \citet{Bengio2009CurriculumL} through the introduction of curriculum learning, suggesting that model performance can improve when tasks are presented in a meaningful order. This principle underlies more recent prompting strategies for large language models, such as least-to-most prompting \citet{zhou2023leasttomostpromptingenablescomplex} and chain-of-thought prompting \citet{wei2023chainofthoughtpromptingelicitsreasoning}, which similarly control information sequencing to guide reasoning.

\subsection{Curriculum Learning}
In the original formulation, \citet{Bengio2009CurriculumL} proposed curriculum learning as a method to gradually increase the complexity of the data samples used during the training process. Inspired by how humans learn, the model starts with simpler tasks and examples, gradually building towards more challenging ones. This approach aims to accelerate convergence, reduce resource consumption, and improve overall performance on complex tasks and examples. The central hypothesis is that by structuring the learning process, curriculum learning can lead to faster training and improved generalization, especially in tasks where traditional random sampling may slow down progress or lead to inefficient learning patterns. \citet{soviany2022curriculumlearningsurvey} found that for NLP tasks, the most commonly used curriculum heuristic tends to be text/question length. However, traditional curriculum learning is centered around creating a learning process analogous to how humans learn, and the text length heuristic is not the best representative of ordering difficulty in a human learning setting.

\subsection{Chain of Thought (CoT)}
Chain of Thought, \citet{wei2023chainofthoughtpromptingelicitsreasoning}, is a series of intermediate reasoning steps and Chain of Thought prompting is a simple prompting technique that significantly improves the ability of large language models to perform complex reasoning and improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks, where a few chain of thought demonstrations are provided as exemplars in prompting. In essence, this works by prompting the LLM to explain its steps in solving a problem and encourages LLMs to show their reasoning by breaking down tasks into smaller pieces.

\subsection{Self-Consistency}
Self-consistency \citet{wang2023selfconsistencyimproveschainthought} is an expansion on the Chain of Thought \cite{wei2023chainofthoughtpromptingelicitsreasoning} prompting method, where the model's decoder is queried (via chain of thought) multiple times. This then generates a diverse set of possible reasoning paths that the model outputs. Finally, the correct answer is chosen by a majority/plurality vote of these outputs. However, this method works and is superior to chain-of-thought reasoning alone in finding the correct answer due to marginalization over the rationale in the chain of thought.

\subsection{Least-to-Most Prompting}
LLMs generally perform poorly when trying to solve problems that are harder than the examples shown in their training prompts. Least-to-Most \citet{zhou2023leasttomostpromptingenablescomplex} prompting is a technique to combat this, where during the chain-of-thought reasoning stage, the model will break a complex problem down into simpler subproblems, using the answers to the subproblems to then derive the answer to the more complex problem. Models prompted with this method are shown in \cite{zhou2023leasttomostpromptingenablescomplex} to be able to generalize solutions to more complex problems than they were trained on.

\subsection{Mixture of Experts (MoE)}
Early foundational efforts explored model composition and ensemble learning. For example, \citet{jacobs1991adaptivemixtureoflocalexperts} proposed adaptive mixtures of local experts, offering a principled way to divide tasks among specialized sub-models (or "experts"), each specializing in a subset of the input data, to jointly perform a task. Mixture of Experts architectures enable large-scale models, even those comprising many billions of parameters, to greatly reduce computation costs during pre-training and achieve faster performance during inference time. Broadly speaking, it achieves this efficiency through selectively activating only the specific experts needed for a given task, rather than activating the entire model for every task. It offers a means to address the tradeoff between the greater capacity of larger models and the greater efficiency of smaller models. This has been most notably explored in the field of natural language processing (NLP): some leading large language models (LLMs) like Mistral’s Mixtral 8x7B and (according to some reports) OpenAI’s GPT-4, have employed MoE architecture. This compositional perspective would later re-emerge in modern architectures where modular reasoning is key. Similarly, \citet{andrew2007scalable} addressed scalability issues in log-linear models through L1 regularization, contributing to efficient training methods that remain relevant today.

In the domain of mathematical reasoning, \citet{hendrycks2021measuringmathematicalproblemsolving} created thee MATH dataset to systematically evaluate problem-solving capabilities, highlighting both the potential and limitations of contemporary models. Later, \citet{cobbe2021trainingverifierssolvemath} trained verifier models to check the validity of generated solutions, reinforcing the idea that auxiliary modules can significantly improve reliability a concept related to our project’s design of intermediate verification mechanism