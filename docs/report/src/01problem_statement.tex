\section{Problem statement}
Despite recent advances in natural language processing, large language models (LLMs) continue to struggle with mathematical reasoning, particularly in tasks that require multi-step logic, abstraction, and generalization across varying difficulty levels. While these models excel in syntactic language generation, their lack of explicit reasoning capabilities often leads to brittle or inconsistent results in math problem-solving. Addressing this gap is crucial not only for enhancing the cognitive capabilities of LLMs but also for enabling practical applications in automated tutoring, education, and scientific problem-solving.

This project investigates whether curriculum learning, a training paradigm inspired by human learning that progresses from simpler to more complex tasks can improve an LLM's mathematical reasoning abilities. Specifically, we propose a structured fine-tuning approach that begins with grade school-level arithmetic (GSM8K dataset) and gradually advances to high school-level competition math (MATH dataset). We complement this strategy with techniques like the Chain of Thought prompting framework to further enhance the modelâ€™s ability to reason systematically.

By comparing multiple fine-tuning strategies, including direct training on complex problems versus a staged curriculum, we aim to identify whether structured learning paths provide measurable benefits in mathematical problem-solving for LLMs. This research contributes to a broader goal of making LLMs more robust and interpretable in domains requiring structured logic and more useful as intelligent educational assistants.
